{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning in OpenAI\n",
    "## Introduction\n",
    "Fine-tuning lets you get more out of the models available through the API by providing:\n",
    "\n",
    "* Higher quality results than prompting\n",
    "* Ability to train on more examples than can fit in a prompt\n",
    "* Token savings due to shorter prompts\n",
    "* Lower latency requests\n",
    "\n",
    "OpenAI's text generation models have been pre-trained on a vast amount of text. To use the models effectively, we include instructions and sometimes several examples in a prompt. Using demonstrations to show how to perform a task is often called \"few-shot learning.\"\n",
    "\n",
    "Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide as many examples in the prompt. This saves costs and enables lower-latency requests.\n",
    "\n",
    "At a high level, fine-tuning involves the following steps:\n",
    "\n",
    "1. Prepare and upload training data\n",
    "2. Train a new fine-tuned model\n",
    "3. Evaluate results and go back to step 1 if needed\n",
    "4. Use your fine-tuned model\n",
    "\n",
    "You can also fine-tune a fine-tuned model which is useful if you acquire additional data and don't want to repeat the previous training steps.\n",
    "We expect gpt-3.5-turbo to be the right model for most users in terms of results and ease of use.\n",
    "\n",
    "## Common Use Cases\n",
    "Some common use cases where fine-tuning can improve results:\n",
    "* Setting the style, tone, format, or other qualitative aspects\n",
    "* Improving reliability at producing a desired output\n",
    "* Correcting failures to follow complex prompts\n",
    "* Handling many edge cases in specific ways\n",
    "* Performing a new skill or task that’s hard to articulate in a prompt\n",
    "\n",
    "One high-level way to think about these cases is when it’s easier to \"show, not tell\". In the sections to come, we will explore how to set up data for fine-tuning and various examples where fine-tuning improves the performance over the baseline model.\n",
    "\n",
    "## Preparing Your Dataset\n",
    "### Example Format\n",
    "Each example in the dataset should be a conversation in the same format as our Chat Completions API, specifically a list of messages where each message has a role, content, and optional name. At least some of the training examples should directly target cases where the prompted model is not behaving as desired, and the provided assistant messages in the data should be the ideal responses you want the model to provide.\n",
    "\n",
    "In this example, our goal is to create a chatbot that occasionally gives sarcastic responses, these are three training examples (conversations) we could create for a dataset:\n",
    "```python\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example count\n",
    "To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples with gpt-3.5-turbo but the right number varies greatly based on the exact use case.\n",
    "\n",
    "### Token limits\n",
    "Token limits depend on the model you select. For gpt-3.5-turbo-1106, the maximum context length is 16,385 so each training example is also limited to 16,385 tokens. To be sure that your entire training example fits in context, consider checking that the total token counts in the message contents are under the limit.\n",
    "\n",
    "### Estimate costs\n",
    "Please refer to the [pricing page](https://openai.com/pricing) for details on cost per 1k input and output tokens (we do to charge for tokens that are part of the validation data). To estimate the costs for a specific fine-tuning job, use the following formula:\n",
    "\n",
    "> base cost per 1k tokens * number of tokens in the input file * number of epochs trained\n",
    "\n",
    "For a training file with 100,000 tokens trained over 3 epochs, the expected cost would be ~$2.40 USD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 10\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a happy assistant that puts a positive spin on everything.'}\n",
      "{'role': 'user', 'content': 'I fell off my bike today.'}\n",
      "{'role': 'assistant', 'content': \"It's great that you're getting exercise outdoors!\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "data_path = \"data/toy_chat_fine_tuning.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format validation\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. **Data Type Check:** Checks whether each entry in the dataset is a dictionary (dict). Error type: **data_type**.\n",
    "2. **Presence of Message List:** Checks if a messages list is present in each entry. Error type: **missing_messages_list**.\n",
    "3. **Message Keys Check:** Validates that each message in the messages list contains the keys role and content. Error type: **message_missing_key**.\n",
    "4. **Unrecognized Keys in Messages:** Logs if a message has keys other than role, content, and name. Error type: **message_unrecognized_key**.\n",
    "5. **Role Validation:** Ensures the role is one of \"system\", \"user\", or \"assistant\". Error type: **unrecognized_role**.\n",
    "6. **Content Validation:** Verifies that content has textual data and is a string. Error type: **missing_content**.\n",
    "7. **Assistant Message Presence:** Checks that each conversation has at least one message from the assistant. Error type: **example_missing_assistant_message**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Counting Utilities\n",
    "Lets define a few helpful utilities to be used in the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken # for token counting\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Warnings and Token Count\n",
    "\n",
    "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
    "\n",
    "1. **Missing System/User Messages:** Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
    "2. **Number of Messages Per Example:** Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
    "3. **Total Tokens Per Example:** Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
    "4. **Tokens in Assistant's Messages:** Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
    "5. **Token Limit Warnings:** Checks if any examples exceed the maximum token limit (4096 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n",
    "\n",
    "Note that token limit for **gpt-3.5-turbo-1106** is 16,385. Token limit for **gpt-3.5-turbo-0613** is 4,096. In this example we will use gpt-3.5-turbo-0613."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 2\n",
      "Num examples missing user message: 3\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 2, 9\n",
      "mean / median: 3.7, 2.5\n",
      "p5 / p95: 2.0, 9.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 26, 8032\n",
      "mean / median: 848.0, 36.5\n",
      "p5 / p95: 26.0, 903.0999999999972\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 4, 8000\n",
      "mean / median: 810.6, 9.5\n",
      "p5 / p95: 4.0, 825.1999999999972\n",
      "\n",
      "1 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Estimation\n",
    "We estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~4544 tokens that will be charged for during training\n",
      "By default, you'll train for 10 epochs on this dataset\n",
      "By default, you'll be charged for ~45440 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Training File\n",
    "Once you have the data validated, the file needs to be uploaded using the [Files API](https://platform.openai.com/docs/api-reference/files/create) in order to be used with a fine-tuning jobs.\n",
    "\n",
    "After you upload the file, it may take some time to process. While the file is processing, you can still create a fine-tuning job but it will not start until the file processing has completed.\n",
    "\n",
    "The maximum file upload size is 1 GB, though we do not suggest fine-tuning with that amount of data since you are unlikely to need that large of an amount to see improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file_object = client.files.create(\n",
    "  file=open(data_path, \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Fine-Tuned Model\n",
    "After ensuring you have the right amount and structure for your dataset, and have uploaded the file, the next step is to create a fine-tuning job. We support creating fine-tuning jobs via the [fine-tuning UI](https://platform.openai.com/finetune) or programmatically.\n",
    "\n",
    "To set additional fine-tuning parameters like the validation_file or hyperparameters, please refer to the [API specification for fine-tuning](https://platform.openai.com/docs/api-reference/fine-tuning/create).\n",
    "\n",
    "After you've started a fine-tuning job, it may take some time to complete. Your job may be queued behind other jobs in our system, and training a model can take minutes or hours depending on the model and dataset size. After the model training is completed, the user who created the fine-tuning job will receive an email confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuning_job = client.fine_tuning.jobs.create(\n",
    "  training_file=file_object.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix=\"for_mlteam\",\n",
    "  hyperparameters={\n",
    "    \"n_epochs\":\"auto\",\n",
    "    \"batch_size\":\"auto\",\n",
    "    \"learning_rate_multiplier\":\"auto\"\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New fine-tuned model 'ft:gpt-3.5-turbo-0613:personal:for-mlteam:8lCxC8vf' created at 2024-01-26 12:32:00 from base model 'gpt-3.5-turbo-0613'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-xXtYbybt4GeiEECdeN17ARny', created_at=1706261906, level='info', message='The job has successfully completed', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-I5jlI7wHztCxRVMTAUmRyatH', created_at=1706261903, level='info', message='New fine-tuned model created: ft:gpt-3.5-turbo-0613:personal:for-mlteam:8lCxC8vf', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-BW4I5iWpJctub4gZ5hZ82Y2f', created_at=1706261883, level='info', message='Step 91/100: training loss=0.00', object='fine_tuning.job.event', data={'step': 91, 'train_loss': 1.430511474609375e-06, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-zF39zWAN4EMK7xlDtNA85woK', created_at=1706261865, level='info', message='Step 81/100: training loss=0.00', object='fine_tuning.job.event', data={'step': 81, 'train_loss': 2.4371677227463806e-06, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-8Ou0MU2hyjtMGemjQofq4IIx', created_at=1706261847, level='info', message='Step 71/100: training loss=0.00', object='fine_tuning.job.event', data={'step': 71, 'train_loss': 2.225239995823358e-06, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-6KMyDCQ9brPD1y9JIj7b5NqF', created_at=1706261827, level='info', message='Step 61/100: training loss=0.00', object='fine_tuning.job.event', data={'step': 61, 'train_loss': 7.311502940865466e-06, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-XMuKYLAIJYgylKDabsE9GBC9', created_at=1706261809, level='info', message='Step 51/100: training loss=0.00', object='fine_tuning.job.event', data={'step': 51, 'train_loss': 1.430511474609375e-05, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-CgZu3Tyi0uT1IvJ2GdmBnk6j', created_at=1706261791, level='info', message='Step 41/100: training loss=0.02', object='fine_tuning.job.event', data={'step': 41, 'train_loss': 0.022484302520751953, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-YGajL3n6SfUWuPpGLewRB6sk', created_at=1706261773, level='info', message='Step 31/100: training loss=0.18', object='fine_tuning.job.event', data={'step': 31, 'train_loss': 0.18139950931072235, 'train_mean_token_accuracy': 1.0}, type='metrics'), FineTuningJobEvent(id='ftevent-k2cynAd73S4NV3ZXM15EmlFT', created_at=1706261754, level='info', message='Step 21/100: training loss=0.46', object='fine_tuning.job.event', data={'step': 21, 'train_loss': 0.46459007263183594, 'train_mean_token_accuracy': 0.8333333134651184}, type='metrics')], object='list', has_more=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In addition to creating a fine-tuning job, you can also list existing jobs, retrieve the status of a job, or cancel a job.\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "# status can be ['validating_files', 'queued', 'running', 'succeeded', 'failed', 'cancelled']\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    fine_tuning_job = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "    if fine_tuning_job.status in ['validating_files', 'queued', 'running']:\n",
    "        continue\n",
    "    if fine_tuning_job.status == \"succeeded\":\n",
    "        created_at = datetime.datetime.fromtimestamp(fine_tuning_job.created_at)\n",
    "        print(f\"New fine-tuned model '{fine_tuning_job.fine_tuned_model}' created at {created_at} from base model '{fine_tuning_job.model}'.\")\n",
    "        break\n",
    "    if fine_tuning_job.status == \"failed\":\n",
    "        print(\"Fine-tuning job failed:\", fine_tuning_job.error)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Fine-tuning job is cancelled.\")\n",
    "        break\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=fine_tuning_job.id, limit=10)\n",
    "\n",
    "# Cancel a job\n",
    "#client.fine_tuning.jobs.cancel(fine_tuning_job.id)\n",
    "\n",
    "# Delete a fine-tuned model (must be an owner of the org the model was created in)\n",
    "#client.models.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:abc123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using your Fine-Tuned Model\n",
    "When a job has succeeded, you will see the fine_tuned_model field populated with the name of the model when you retrieve the job details. You may now specify this model as a parameter to in the Chat Completions API.\n",
    "\n",
    "After your job is completed, the model should be available right away for inference use. In some cases, it may take several minutes for your model to become ready to handle requests. If requests to your model time out or the model name cannot be found, it is likely because your model is still being loaded. If this happens, try again in a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "  model=fine_tuning_job.fine_tuned_model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing your Fine-Tuned Model\n",
    "\n",
    "API provides the following training metrics computed over the course of training: training loss, training token accuracy, test loss, and test token accuracy. These statistics are meant to provide a sanity check that training went smoothly (loss should decrease, token accuracy should increase). While an active fine-tuning jobs is running, you can view an event object which contains some useful metrics:\n",
    "```python\n",
    "{\n",
    "    \"object\": \"fine_tuning.job.event\",\n",
    "    \"id\": \"ftevent-abc-123\",\n",
    "    \"created_at\": 1693582679,\n",
    "    \"level\": \"info\",\n",
    "    \"message\": \"Step 100/100: training loss=0.00\",\n",
    "    \"data\": {\n",
    "        \"step\": 100,\n",
    "        \"train_loss\": 1.805623287509661e-5,\n",
    "        \"train_mean_token_accuracy\": 1.0\n",
    "    },\n",
    "    \"type\": \"metrics\"\n",
    "}\n",
    "```\n",
    "After a fine-tuning job has finished, you can also see metrics around how the training process went by querying a fine-tuning job, extracting a file ID from the result_files, and then retrieving that files content. Each results CSV file has the following columns: step, train_loss, train_accuracy, valid_loss, and valid_mean_token_accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   step  train_loss  train_accuracy  valid_loss  valid_mean_token_accuracy\n",
      "0     1     5.71738         0.27273         NaN                        NaN\n",
      "1     2     3.42625         0.33333         NaN                        NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fine_tuning_job = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "\n",
    "for i, file_id in enumerate(fine_tuning_job.result_files):\n",
    "    content = client.files.content(file_id)\n",
    "    # save content to file\n",
    "    with open(f\"output/fine_tuning_result{i}.csv\", \"wb\") as f:\n",
    "        f.write(content.text.encode(\"utf-8\"))\n",
    "    df = pd.read_csv(f\"output/fine_tuning_result{i}.csv\")\n",
    "    print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While metrics can he helpful, evaluating samples from the fine-tuned model provides the most relevant sense of model quality. We recommend generating samples from both the base model and the fine-tuned model on a test set, and comparing the samples side by side. The test set should ideally include the full distribution of inputs that you might send to the model in a production use case. If manual evaluation is too time-consuming, consider using our [Evals library](https://github.com/openai/evals) to automate future evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Go further for Evals library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate\n",
    "If the results from a fine-tuning job are not as good as you expected, continue to iterate:\n",
    "1. Iterate on data quality\n",
    "    * Check if your model has grammar, logic, or style issues\n",
    "    * Consider the balance and diversity of data\n",
    "    * Make sure your training examples contain all of the information needed for the response\n",
    "    * Look at the consistency in the training examples\n",
    "    * Make sure your all of your training examples are in the same format, as expected for inference\n",
    "2. Iterate on data quantity\n",
    "    \n",
    "    Once you’re satisfied with the quality and distribution of the examples, you can consider scaling up the number of training examples. This tends to help the model learn the task better, especially around possible \"edge cases\". \n",
    "3. Iterate on hyperparameters\n",
    "    \n",
    "    We allow you to specify the following hyperparameters:\n",
    "    * **epochs:** If the model does not follow the training data as much as expected increase the number of epochs by 1 or 2. If the model becomes less diverse than expected decrease the number of epochs by 1 or 2\n",
    "    * **learning rate multiplier:** If the model does not appear to be converging, increase the learning rate multiplier\n",
    "    * **batch size:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning with Function Calls\n",
    "Including a long list of functions in the completions API can consume a considerable number of prompt tokens and sometimes the model hallucinates or does not provide valid JSON output.\n",
    "\n",
    "Fine-tuning a model with function calling examples can allow you to:\n",
    "\n",
    "* Get similarly formatted responses even when the full function definition isn't present\n",
    "* Get more accurate and consistent outputs\n",
    "\n",
    "Fine-tuning on function calling can also be used to customize the model's response to function outputs. To do this you can include a function response message and an assistant message interpreting that response.\n",
    "\n",
    "NOTE: Remaining part of the tutorial will not run, because when we try to upload the training data ('data/weather_chat_fine_tuning.jsonl'), file API validation rules fail. But, actually the file content is valid and compatible with the latest function calling API. Unfortunately file validation rules are not up-to-date and they do not support the latest function calling API standard as of January 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training data\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file_object = client.files.create(\n",
    "  file=open(\"data/weather_chat_fine_tuning.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a fine tuning job\n",
    "fine_tuning_job = client.fine_tuning.jobs.create(\n",
    "  training_file=file_object.id, \n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix=\"weather\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait till the new model is created\n",
    "while True:\n",
    "    time.sleep(5)\n",
    "    fine_tuning_job = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "    if fine_tuning_job.status in ['validating_files', 'queued', 'running']:\n",
    "        continue\n",
    "    if fine_tuning_job.status == \"succeeded\":\n",
    "        break\n",
    "    if fine_tuning_job.status == \"failed\":\n",
    "        print(\"Fine-tuning job failed:\", fine_tuning_job.error)\n",
    "        break\n",
    "    else:\n",
    "        print(\"Fine-tuning job is cancelled.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "import json\n",
    "\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: send the conversation and available functions to the model\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Istanbul?\"}]\n",
    "tools = [\n",
    "{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "]\n",
    "\n",
    "# Use the fine-tuned model\n",
    "response = client.chat.completions.create(\n",
    "    model=fine_tuning_job.fine_tuned_model,\n",
    "    messages=messages,\n",
    "    tools=tools\n",
    ")\n",
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "# Step 2: check if the model wanted to call a function\n",
    "if tool_calls:\n",
    "    # Step 3: call the function\n",
    "    # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "    available_functions = {\n",
    "        \"get_current_weather\": get_current_weather,\n",
    "    }  # only one function in this example, but you can have multiple\n",
    "    messages.append(response_message)  # extend conversation with assistant's reply\n",
    "    # Step 4: send the info for each function call and function response to the model\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        function_response = function_to_call(\n",
    "            location=function_args.get(\"location\"),\n",
    "            unit=function_args.get(\"unit\"),\n",
    "        )\n",
    "        messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )  # extend conversation with function response\n",
    "    # Again use the fine-tuned model\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=fine_tuning_job.fine_tuned_model,\n",
    "        messages=messages,\n",
    "    )  # get a new response from the model where it can see the function response\n",
    "    print(second_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
