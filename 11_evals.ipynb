{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Evals\n",
    "\n",
    "[Evals](https://github.com/openai/evals) provide a framework for evaluating large language models (LLMs) or systems built using LLMs. It offers an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.\n",
    "\n",
    "If you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might effect your use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "There are two different options to install 'evals':\n",
    "1. Cloning 'evals' repository and installing from the source code\n",
    "2. Using pip install\n",
    "\n",
    "Theoritically option 2 should be sufficient to use evals framework and run your evaluations. But I recommend the option 1 for the several reasons:\n",
    "* The version in the Python Package Index is out-of-date, it is not compatible with the lastest Python library as of Feb 2024. This migth be the case when you try.\n",
    "* To conveniently browse the source code, check the examples, read the documents, see the JSONL file contents it is good to have the source codes.\n",
    "* 'evals' is open to contribution. You may want to contribute in your next step.\n",
    "\n",
    "### Option 1: Use the source code\n",
    "Evals registry is stored using [Git-LFS](https://git-lfs.com/). First install LFS.\n",
    "```console\n",
    "brew install git-lfs\n",
    "git lfs install\n",
    "```\n",
    "\n",
    "Go to parent directory (Assuming this notebook is in openai-cheat-sheet/) and clone evals repository\n",
    "```console\n",
    "cd ..\n",
    "git clone https://github.com/openai/evals.git\n",
    "```\n",
    "\n",
    "Fetch all the files (from within your local copy of the evals repo). This will populate all the pointer files under **evals/registry/data**.\n",
    "```console\n",
    "cd ../evals\n",
    "git lfs fetch --all \n",
    "git lfs pull\n",
    "```\n",
    "\n",
    "Install evals from the source code along with its dependencies.\n",
    "```console\n",
    "cd ../evals\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "### Option 2: Install from Python Package Index\n",
    "\n",
    "```console\n",
    "pip install evals\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario: Evaluation of retriaval with different options\n",
    "\n",
    "Let's assume that we have a chat completion that uses a CSV file for retriaval. This file should have only two columns: text and embedding. For each user query, it will find top {k} embeddings from CSV, which are the closest to the user's query, then add the corresponding 'text' of these embeddings to the system message to enrich the context. And the completion will reply accordingly. \n",
    "\n",
    "This is already implemented in [evals.completion_fns.retrieval:RetrievalCompletionFn](https://github.com/openai/evals/blob/main/evals/completion_fns/retrieval.py) class in evals. But in my tests, it was not working correctly. So here we will create our own custom completion function and register it.\n",
    "\n",
    "Our aim is to try out 3 different options with the same sample data set and compare their accuracy scores.\n",
    "1. Using GPT-3.5-turbo directly without feeding any extra context with retrieval.\n",
    "2. Using GPT-3.5-turbo with retrieval.\n",
    "3. Using GPT-3.5-turbo with retrieval and instructing it to use chain-of-thought logic.\n",
    "\n",
    "For this, we will use a file with the birthdays of all presidents. Our completion function will retrieve the data from this file. Then we will test it with some sample user questions like \"Was Franklin Pierce born before Abraham Lincoln? Answer Y or N.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup retrieval data\n",
    "While we are using RetrievalCompletionFn, we will use [president_birthdays.csv](./data/president_birthdays.csv). \n",
    "1. We will generate the 'text' column using the data in the file\n",
    "2. We will get the 'embeddings' using the text column\n",
    "3. We will generate 'output/presidents_embeddings.csv' file only with 'text' and 'embedding' columns. This file will be the input for RetrievalCompletionFn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"George Washington\"</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1732.0</td>\n",
       "      <td>\"George Washington\" was born on  2/22/1732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"John Adams\"</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>1735.0</td>\n",
       "      <td>\"John Adams\" was born on  10/30/1735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Thomas Jefferson\"</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>\"Thomas Jefferson\" was born on  4/13/1743.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"James Madison\"</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1751.0</td>\n",
       "      <td>\"James Madison\" was born on  3/16/1751.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"James Monroe\"</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>1758.0</td>\n",
       "      <td>\"James Monroe\" was born on  4/28/1758.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  Day Month    Year  \\\n",
       "Index                                            \n",
       "1       \"George Washington\"   22     2  1732.0   \n",
       "2              \"John Adams\"   30    10  1735.0   \n",
       "3        \"Thomas Jefferson\"   13     4  1743.0   \n",
       "4           \"James Madison\"   16     3  1751.0   \n",
       "5            \"James Monroe\"   28     4  1758.0   \n",
       "\n",
       "                                                text  \n",
       "Index                                                 \n",
       "1       \"George Washington\" was born on  2/22/1732.0  \n",
       "2             \"John Adams\" was born on  10/30/1735.0  \n",
       "3        \"Thomas Jefferson\" was born on  4/13/1743.0  \n",
       "4           \"James Madison\" was born on  3/16/1751.0  \n",
       "5            \"James Monroe\" was born on  4/28/1758.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_datapath = \"data/president_birthdays.csv\"\n",
    "\n",
    "df = pd.read_csv(input_datapath).rename(columns={\" \\\"Name\\\"\": \"Name\", \" \\\"Month\\\"\": \"Month\", \" \\\"Day\\\"\": \"Day\", \" \\\"Year\\\"\": \"Year\"}).set_index(\"Index\")\n",
    "df[\"text\"] = df.apply(lambda r: f\"{r['Name']} was born on {r['Month']}/{r['Day']}/{r['Year']}\", axis=1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def embed(text):\n",
    "    return client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        ).data[0].embedding\n",
    "\n",
    "df[\"embedding\"] = df['text'].apply(embed)\n",
    "df[[\"text\", \"embedding\"]].to_csv(\"output/presidents_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement your completion function\n",
    "\n",
    "You can check [MLRetrievalCompletionFn class](./evals_registry/completion_fns/retrieval.py). It accepts 3 arguments.\n",
    "1. **completion_fn**: we can simply pass a model name, e.g. 'gpt-4-turbo' or pass another completions function. It is kind of chain, a completion function having another one as an input. \n",
    "2. **embeddings_and_text_path**: CSV file with 'text' and 'embedding' columns. It will be used for retrieval. \n",
    "3. **k**: Top k closest embeddings will be passed to the prompt. In our case it is 2, because the user will always ask questions similar to \"Was Andrew Jackson born before William Harrison?\", so we always need only two president's data.\n",
    "\n",
    "If you want to read more about Completion Functions, you can read [this document](https://github.com/openai/evals/blob/main/docs/completion-fns.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Register your completion function\n",
    "\n",
    "To register our completion function, we need to write a YAML file. You can check [presidents.yaml](./evals_registry/completion_fns/presidents.yaml). \n",
    "\n",
    "Here we defined 3 different completion functions:\n",
    "1. **cot/gpt-3.5**: We will not use this one directly, but use it as an input to the 3rd one. It is based on [ChainOfThoughtCompletionFn](https://github.com/openai/evals/blob/main/evals/completion_fns/cot.py) class, adds chain of thought logic on top of gpt-3.5-turbo.\n",
    "1. **retrieval/presidents/gpt-3.5-turbo**: for using 'gpt-3.5-turbo' with retrieval.\n",
    "2. **retrieval/presidents/cot/gpt-3.5-turbo**: for using 'gpt-3.5-turbo' with chain of thought logic, with retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cot/gpt-3.5:\n",
      "  class: evals.completion_fns.cot:ChainOfThoughtCompletionFn\n",
      "  args:\n",
      "    cot_completion_fn: gpt-3.5-turbo\n",
      "\n",
      "retrieval/presidents/gpt-3.5-turbo:\n",
      "  class: evals_registry.completion_fns.retrieval:MLRetrievalCompletionFn\n",
      "  args:\n",
      "    completion_fn: gpt-3.5-turbo\n",
      "    embeddings_and_text_path: output/presidents_embeddings.csv\n",
      "    k: 2\n",
      "\n",
      "retrieval/presidents/cot/gpt-3.5-turbo:\n",
      "  class: evals_registry.completion_fns.retrieval:MLRetrievalCompletionFn\n",
      "  args:\n",
      "    completion_fn: cot/gpt-3.5\n",
      "    embeddings_and_text_path: output/presidents_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open('evals_registry/completion_fns/presidents.yaml', 'r') as file:\n",
    "    # Read the file's content\n",
    "    file_content = file.read()\n",
    "    # Print the content\n",
    "    print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build your eval\n",
    "\n",
    "An eval is simply a dataset and a choice of eval class. You have 3 options to build an eval:\n",
    "1. Using one of the basic eval classes. Most common ones are 'Match', 'Includes' and 'FuzyMatch'. For a full list, you can check [here](https://github.com/openai/evals/tree/main/evals/elsuite/basic).\n",
    "2. Using the [model graded eval class](https://github.com/openai/evals/blob/main/docs/eval-templates.md#the-model-graded-eval-template). Model graded eval means using an LLM model to evaluate the outputs of another LLM model. \n",
    "3. Creating your custom eval class. Most of the cases, this will not be necessary. So it is not covered under this notebook. But you can check [here](https://github.com/openai/evals/blob/main/docs/custom-eval.md), if you want to learn more.\n",
    "\n",
    "Register the eval by adding a file to /evals/<eval_name>.yaml under registry folder (in our case it is 'evals_registry') using the elsuite registry format. For example, for a Match eval, it would be:\n",
    "\n",
    "```console\n",
    "<eval_name>:\n",
    "  id: <eval_name>.dev.v0\n",
    "  description: <description>\n",
    "  metrics: [accuracy]\n",
    "\n",
    "<eval_name>.dev.v0:\n",
    "  class: evals.elsuite.basic.match:Match\n",
    "  args:\n",
    "    samples_jsonl: <eval_name>/samples.jsonl\n",
    "```\n",
    "\n",
    "Upon running the eval, the data will be searched for in 'data' folder under registry. For example, if older/samples.jsonl is the provided file, the data is expected to be in evals_registry/data/older/samples.jsonl.\n",
    "\n",
    "The naming convention for evals is in the form {eval_name}.{split>}.{version}\n",
    "\n",
    "* **eval_name** is the eval name, used to group evals whose scores are comparable.\n",
    "* **split** is the data split, used to further group evals that are under the same <base_eval>. E.g., \"val\", \"test\", or \"dev\" for testing.\n",
    "* **version** is the version of the eval, which can be any descriptive text you'd like to use (though it's best if it does not contain .).\n",
    "\n",
    "In general, running the same eval name against the same model should always give similar results so that others can reproduce it. Therefore, when you change your eval, you should bump the version.\n",
    "\n",
    "In our sample scerio, we will go ahead with the first option and use 'Match'. You can check [older.yaml](./evals_registry/evals/older.yaml). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "older:\n",
      "  id: older.dev.v0\n",
      "  description: Test the model's ability to determine who is older.\n",
      "  metrics: [accuracy]\n",
      "older.dev.v0:\n",
      "  class: evals.elsuite.basic.match:Match\n",
      "  args:\n",
      "    samples_jsonl: older/samples.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open('evals_registry/evals/older.yaml', 'r') as file:\n",
    "    # Read the file's content\n",
    "    file_content = file.read()\n",
    "    # Print the content\n",
    "    print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Setup your sample data to use for evaluation\n",
    "You will need to convert your samples into the right JSON lines (JSONL) format. A JSONL file is just a JSON file with a unique JSON object per line.\n",
    "\n",
    "You can use the openai CLI (available with OpenAI-Python) to transform data from some common file types into JSONL:\n",
    "\n",
    "```console\n",
    "openai tools fine_tunes.prepare_data -f data[.csv, .json, .txt, .xlsx or .tsv]\n",
    "```\n",
    "You can find some examples of JSONL eval files [here](https://github.com/openai/evals/blob/main/evals/registry/data/README.md)\n",
    "\n",
    "Each JSON object will represent one data point in your eval. The keys you need in the JSON object depend on the eval template. All templates expect an \"`input`\" key, which is the prompt.\n",
    "\n",
    "For the basic evals Match, Includes, and FuzzyMatch, the other required key is \"`ideal`\", which is a string (or a list of strings) specifying the correct reference answer(s). \n",
    "\n",
    "In our sample scerio, we will use [samples.jsonl](./evals_registry/data/older/samples.jsonl). Check the content of it to have an understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Was Abraham Lincoln born before Franklin Pierce? Answer Y or N.\"}], \"ideal\": \"N\"}\n",
      "{\"input\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Was Abraham Lincoln born before Andrew Johnson? Answer Y or N.\"}], \"ideal\": \"N\"}\n",
      "{\"input\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Was Andrew Jackson born before John Quincy Adams? Answer Y or N.\"}], \"ideal\": \"Y\"}\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open('evals_registry/data/older/samples.jsonl', 'r') as file:\n",
    "    # Loop over the first 3 lines and print each\n",
    "    for i, line in enumerate(file):\n",
    "        if i < 3:\n",
    "            print(line, end='')  # Use end='' to avoid adding extra newlines\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Run the eval\n",
    "After installing evals tool, we can simply run evaluation in the command line by defining completion function and evaluation task.\n",
    "\n",
    "* `completion_fn`: If you will only use an OpenAI model directly, you simply put its name. If you have your own custom completion function (as in [presidents.yaml](./evals_registry/completion_fns/presidents.yaml)), put its name.\n",
    "* `eval_task`: It refers to a YAML file in the registry directory. The file defines parameters for a specific evaluation task, e.g., evaluation data, evaluation metrics and prompting strategies. \n",
    "\n",
    "```console\n",
    "oaieval <completion_fn> <eval_task>\n",
    "```\n",
    "\n",
    "Each run will create a jsonl log file under the tmp folder as such '/tmp/evallogs/2402140749577WDOLKUD_gpt-3.5-turbo_older.jsonl'. You can check the corresponding file to see how the run went.\n",
    "\n",
    "For more information on running evals, you can read [this](https://github.com/openai/evals/blob/main/docs/run-evals.md).\n",
    "\n",
    "**NOTE**: The default registry path for evals is 'evals/registry', here we use a custom path of './evals_registry'. So we need to pass `--registry_path` argument to point out our registry folder.\n",
    "\n",
    "**NOTE**: For evals to resolve evals_registry.completion_fns.retrieval:MLRetrievalCompletionFn class, we need to add the path of the evals_registry folder to the PYTHONPATH. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-28 16:59:48,590] [registry.py:271] Loading registry from /Users/meltemseyhan/M9/MLTeam/mlteam-openai-training/openai-cheat-sheet/env/lib/python3.12/site-packages/evals/registry/evals\n",
      "[2024-04-28 16:59:49,134] [registry.py:271] Loading registry from /Users/meltemseyhan/.evals/evals\n",
      "[2024-04-28 16:59:49,134] [registry.py:271] Loading registry from evals_registry/evals\n",
      "[2024-04-28 16:59:49,137] [oaieval.py:215] \u001b[1;35mRun started: 240428135949DJTNETK7\u001b[0m\n",
      "[2024-04-28 16:59:49,211] [data.py:94] Fetching evals_registry/data/older/samples.jsonl\n",
      "[2024-04-28 16:59:49,212] [eval.py:36] Evaluating 10 samples\n",
      "[2024-04-28 16:59:49,253] [eval.py:144] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.73it/s]\n",
      "[2024-04-28 16:59:51,956] [oaieval.py:275] Found 10/10 sampling events with usage data\n",
      "[2024-04-28 16:59:51,956] [oaieval.py:283] Token usage from 10 sampling events:\n",
      "completion_tokens: 12\n",
      "prompt_tokens: 317\n",
      "total_tokens: 329\n",
      "[2024-04-28 16:59:51,957] [record.py:371] Final report: {'accuracy': 0.7, 'boostrap_std': 0.15648015848662733, 'usage_completion_tokens': 12, 'usage_prompt_tokens': 317, 'usage_total_tokens': 329}. Logged to /tmp/evallogs/240428135949DJTNETK7_gpt-3.5-turbo_older.jsonl\n",
      "[2024-04-28 16:59:51,957] [oaieval.py:233] Final report:\n",
      "[2024-04-28 16:59:51,957] [oaieval.py:235] accuracy: 0.7\n",
      "[2024-04-28 16:59:51,957] [oaieval.py:235] boostrap_std: 0.15648015848662733\n",
      "[2024-04-28 16:59:51,957] [oaieval.py:235] usage_completion_tokens: 12\n",
      "[2024-04-28 16:59:51,957] [oaieval.py:235] usage_prompt_tokens: 317\n",
      "[2024-04-28 16:59:51,957] [oaieval.py:235] usage_total_tokens: 329\n",
      "[2024-04-28 16:59:51,962] [record.py:360] Logged 20 rows of events to /tmp/evallogs/240428135949DJTNETK7_gpt-3.5-turbo_older.jsonl: insert_time=3.746ms\n"
     ]
    }
   ],
   "source": [
    "# Use gpt-3.5-turbo directly without any retrieval or extra prompt -> Accuracy: 0.7\n",
    "!export PYTHONPATH=\".:$PYTHONPATH\"; oaieval gpt-3.5-turbo older --max_samples 10 --registry_path ./evals_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-28 17:00:18,157] [registry.py:271] Loading registry from /Users/meltemseyhan/M9/MLTeam/mlteam-openai-training/openai-cheat-sheet/env/lib/python3.12/site-packages/evals/registry/evals\n",
      "[2024-04-28 17:00:18,756] [registry.py:271] Loading registry from /Users/meltemseyhan/.evals/evals\n",
      "[2024-04-28 17:00:18,756] [registry.py:271] Loading registry from evals_registry/evals\n",
      "[2024-04-28 17:00:19,266] [registry.py:271] Loading registry from /Users/meltemseyhan/M9/MLTeam/mlteam-openai-training/openai-cheat-sheet/env/lib/python3.12/site-packages/evals/registry/completion_fns\n",
      "[2024-04-28 17:00:19,271] [registry.py:271] Loading registry from /Users/meltemseyhan/.evals/completion_fns\n",
      "[2024-04-28 17:00:19,271] [registry.py:271] Loading registry from evals_registry/completion_fns\n",
      "[2024-04-28 17:00:19,273] [registry.py:271] Loading registry from /Users/meltemseyhan/M9/MLTeam/mlteam-openai-training/openai-cheat-sheet/env/lib/python3.12/site-packages/evals/registry/solvers\n",
      "[2024-04-28 17:00:19,455] [registry.py:271] Loading registry from /Users/meltemseyhan/.evals/solvers\n",
      "[2024-04-28 17:00:19,455] [registry.py:271] Loading registry from evals_registry/solvers\n",
      "[2024-04-28 17:00:19,727] [utils.py:148] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[2024-04-28 17:00:19,727] [utils.py:161] NumExpr defaulting to 8 threads.\n",
      "[2024-04-28 17:00:27,157] [oaieval.py:215] \u001b[1;35mRun started: 2404281400273Z3FY4CV\u001b[0m\n",
      "[2024-04-28 17:00:27,158] [data.py:94] Fetching evals_registry/data/older/samples.jsonl\n",
      "[2024-04-28 17:00:27,158] [eval.py:36] Evaluating 10 samples\n",
      "[2024-04-28 17:00:27,171] [eval.py:144] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  3.04it/s]\n",
      "[2024-04-28 17:00:30,482] [oaieval.py:275] Found 10/20 sampling events with usage data\n",
      "[2024-04-28 17:00:30,483] [oaieval.py:283] Token usage from 10 sampling events:\n",
      "completion_tokens: 13\n",
      "prompt_tokens: 831\n",
      "total_tokens: 844\n",
      "[2024-04-28 17:00:30,483] [record.py:371] Final report: {'accuracy': 0.9, 'boostrap_std': 0.09993517899118406, 'usage_completion_tokens': 13, 'usage_prompt_tokens': 831, 'usage_total_tokens': 844}. Logged to /tmp/evallogs/2404281400273Z3FY4CV_retrieval/presidents/gpt-3.5-turbo_older.jsonl\n",
      "[2024-04-28 17:00:30,483] [oaieval.py:233] Final report:\n",
      "[2024-04-28 17:00:30,484] [oaieval.py:235] accuracy: 0.9\n",
      "[2024-04-28 17:00:30,484] [oaieval.py:235] boostrap_std: 0.09993517899118406\n",
      "[2024-04-28 17:00:30,484] [oaieval.py:235] usage_completion_tokens: 13\n",
      "[2024-04-28 17:00:30,484] [oaieval.py:235] usage_prompt_tokens: 831\n",
      "[2024-04-28 17:00:30,484] [oaieval.py:235] usage_total_tokens: 844\n",
      "[2024-04-28 17:00:30,492] [record.py:360] Logged 30 rows of events to /tmp/evallogs/2404281400273Z3FY4CV_retrieval/presidents/gpt-3.5-turbo_older.jsonl: insert_time=5.034ms\n"
     ]
    }
   ],
   "source": [
    "# Use gpt-3.5-turbo with retrieval -> Accuracy: 0.9\n",
    "!export PYTHONPATH=\".:$PYTHONPATH\"; oaieval retrieval/presidents/gpt-3.5-turbo older --max_samples 10 --registry_path ./evals_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-28 17:00:51,275] [registry.py:271] Loading registry from /Users/meltemseyhan/M9/MLTeam/mlteam-openai-training/openai-cheat-sheet/env/lib/python3.12/site-packages/evals/registry/evals\n",
      "[2024-04-28 17:00:51,824] [registry.py:271] Loading registry from /Users/meltemseyhan/.evals/evals\n",
      "[2024-04-28 17:00:51,824] [registry.py:271] Loading registry from evals_registry/evals\n",
      "[2024-04-28 17:00:52,299] [registry.py:271] Loading registry from /Users/meltemseyhan/M9/MLTeam/mlteam-openai-training/openai-cheat-sheet/env/lib/python3.12/site-packages/evals/registry/completion_fns\n",
      "[2024-04-28 17:00:52,305] [registry.py:271] Loading registry from /Users/meltemseyhan/.evals/completion_fns\n",
      "[2024-04-28 17:00:52,305] [registry.py:271] Loading registry from evals_registry/completion_fns\n",
      "[2024-04-28 17:00:52,307] [registry.py:271] Loading registry from /Users/meltemseyhan/M9/MLTeam/mlteam-openai-training/openai-cheat-sheet/env/lib/python3.12/site-packages/evals/registry/solvers\n",
      "[2024-04-28 17:00:52,491] [registry.py:271] Loading registry from /Users/meltemseyhan/.evals/solvers\n",
      "[2024-04-28 17:00:52,491] [registry.py:271] Loading registry from evals_registry/solvers\n",
      "[2024-04-28 17:00:52,786] [utils.py:148] Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[2024-04-28 17:00:52,786] [utils.py:161] NumExpr defaulting to 8 threads.\n",
      "[2024-04-28 17:00:53,380] [oaieval.py:215] \u001b[1;35mRun started: 240428140053WTGCZT6J\u001b[0m\n",
      "[2024-04-28 17:00:53,381] [data.py:94] Fetching evals_registry/data/older/samples.jsonl\n",
      "[2024-04-28 17:00:53,382] [eval.py:36] Evaluating 10 samples\n",
      "[2024-04-28 17:00:53,393] [eval.py:144] Running in threaded mode with 10 threads!\n",
      "100%|███████████████████████████████████████████| 10/10 [00:04<00:00,  2.32it/s]\n",
      "[2024-04-28 17:00:57,731] [oaieval.py:275] Found 20/50 sampling events with usage data\n",
      "[2024-04-28 17:00:57,732] [oaieval.py:283] Token usage from 20 sampling events:\n",
      "completion_tokens: 945\n",
      "prompt_tokens: 4,193\n",
      "total_tokens: 5,138\n",
      "[2024-04-28 17:00:57,732] [record.py:371] Final report: {'accuracy': 0.9, 'boostrap_std': 0.09999499987499373, 'usage_completion_tokens': 945, 'usage_prompt_tokens': 4193, 'usage_total_tokens': 5138}. Logged to /tmp/evallogs/240428140053WTGCZT6J_retrieval/presidents/cot/gpt-3.5-turbo_older.jsonl\n",
      "[2024-04-28 17:00:57,732] [oaieval.py:233] Final report:\n",
      "[2024-04-28 17:00:57,732] [oaieval.py:235] accuracy: 0.9\n",
      "[2024-04-28 17:00:57,732] [oaieval.py:235] boostrap_std: 0.09999499987499373\n",
      "[2024-04-28 17:00:57,732] [oaieval.py:235] usage_completion_tokens: 945\n",
      "[2024-04-28 17:00:57,732] [oaieval.py:235] usage_prompt_tokens: 4193\n",
      "[2024-04-28 17:00:57,733] [oaieval.py:235] usage_total_tokens: 5138\n",
      "[2024-04-28 17:00:57,745] [record.py:360] Logged 60 rows of events to /tmp/evallogs/240428140053WTGCZT6J_retrieval/presidents/cot/gpt-3.5-turbo_older.jsonl: insert_time=10.096ms\n"
     ]
    }
   ],
   "source": [
    "# Use gpt-3.5-turbo with retrieval and chain-of-thought -> Accuracy: 0.9\n",
    "!export PYTHONPATH=\".:$PYTHONPATH\"; oaieval retrieval/presidents/cot/gpt-3.5-turbo older --max_samples 10 --registry_path ./evals_registry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
