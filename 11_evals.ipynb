{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Evals\n",
    "\n",
    "[Evals](https://github.com/openai/evals) provide a framework for evaluating large language models (LLMs) or systems built using LLMs. It offers an existing registry of evals to test different dimensions of OpenAI models and the ability to write your own custom evals for use cases you care about. You can also use your data to build private evals which represent the common LLMs patterns in your workflow without exposing any of that data publicly.\n",
    "\n",
    "If you are building with LLMs, creating high quality evals is one of the most impactful things you can do. Without evals, it can be very difficult and time intensive to understand how different model versions might effect your use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Use of evals\n",
    "After installing this tool, we can simply run evaluation in the command line by defining completion function (the use of models; prompting strategies…) and evaluation task (evaluation metrics, datasets and general protocols).\n",
    "\n",
    "* `completion_fn`: I only want to evaluate openai Chat LLMs. Therefore, I can use any openai model id, e.g., “gpt-3.5-turbo”, “gpt-4”, “gpt-4–32k”. Here, I use gpt-3.5-turbo . But the evals framework provides general protocols for other LLM piplines and names them as completion functions, e.g., LangChain LLMs.\n",
    "* `eval_task`: It refers to a YAML file in the evals.registry.evals directory. The file defines parameters for a specific evaluation task, e.g., evaluation data, evaluation metrics and prompting strategies. You can refer to Section 1: Specification File for details. Here, match_mmlu_machine_learning refers to a specification file discussed in the next bullet point.\n",
    "```console\n",
    "oaieval gpt-3.5-turbo test-match\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Evalution of retriaval with different options\n",
    "\n",
    "Let's assume that we have a chat completion that uses a CSV file for retriaval. This file should have only two columns: text and embedding. For each user query, it will find top {k} embeddings from CSV, which are the closest to the user's query, then add the corresponding 'text' of these embeddings to the system message to enrich the context. And the completion will reply accordingly. This is already implemented in [evals.completion_fns.retrieval:RetrievalCompletionFn](https://github.com/openai/evals/blob/main/evals/completion_fns/retrieval.py) class in evals.\n",
    "\n",
    "We will register our own completion function using this class. It is also possible to implement a custom completion function class that inherits from events.api:CompletionFn. In this example, we will just use the existing 'RetrievalCompletionFn'. If you want to read more about Completion Functions, you can read [this document](https://github.com/openai/evals/blob/main/docs/completion-fns.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup retrieval data\n",
    "While we are using RetrievalCompletionFn, we will use [president_birthdays.csv](./data/president_birthdays.csv). \n",
    "1. We will generate the 'text' column using the data in the file\n",
    "2. We will will get the 'embeddings' using the text column\n",
    "3. We will generate 'output/presidents_embeddings.csv' file only with 'text' and 'embedding' columns. This file will be the input for RetrievalCompletionFn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"George Washington\"</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1732.0</td>\n",
       "      <td>\"George Washington\" was born on  2/22/1732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"John Adams\"</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>1735.0</td>\n",
       "      <td>\"John Adams\" was born on  10/30/1735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Thomas Jefferson\"</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>\"Thomas Jefferson\" was born on  4/13/1743.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"James Madison\"</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1751.0</td>\n",
       "      <td>\"James Madison\" was born on  3/16/1751.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"James Monroe\"</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>1758.0</td>\n",
       "      <td>\"James Monroe\" was born on  4/28/1758.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  Day Month    Year  \\\n",
       "Index                                            \n",
       "1       \"George Washington\"   22     2  1732.0   \n",
       "2              \"John Adams\"   30    10  1735.0   \n",
       "3        \"Thomas Jefferson\"   13     4  1743.0   \n",
       "4           \"James Madison\"   16     3  1751.0   \n",
       "5            \"James Monroe\"   28     4  1758.0   \n",
       "\n",
       "                                                text  \n",
       "Index                                                 \n",
       "1       \"George Washington\" was born on  2/22/1732.0  \n",
       "2             \"John Adams\" was born on  10/30/1735.0  \n",
       "3        \"Thomas Jefferson\" was born on  4/13/1743.0  \n",
       "4           \"James Madison\" was born on  3/16/1751.0  \n",
       "5            \"James Monroe\" was born on  4/28/1758.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_datapath = \"data/president_birthdays.csv\"\n",
    "\n",
    "df = pd.read_csv(input_datapath).rename(columns={\" \\\"Name\\\"\": \"Name\", \" \\\"Month\\\"\": \"Month\", \" \\\"Day\\\"\": \"Day\", \" \\\"Year\\\"\": \"Year\"}).set_index(\"Index\")\n",
    "df[\"text\"] = df.apply(lambda r: f\"{r['Name']} was born on {r['Month']}/{r['Day']}/{r['Year']}\", axis=1)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"George Washington\"</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1732.0</td>\n",
       "      <td>\"George Washington\" was born on  2/22/1732.0</td>\n",
       "      <td>[-0.0066544716246426105, 0.0005056292284280062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"John Adams\"</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>1735.0</td>\n",
       "      <td>\"John Adams\" was born on  10/30/1735.0</td>\n",
       "      <td>[-0.0015555905411019921, 0.040288060903549194,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name  Day Month    Year  \\\n",
       "Index                                            \n",
       "1       \"George Washington\"   22     2  1732.0   \n",
       "2              \"John Adams\"   30    10  1735.0   \n",
       "\n",
       "                                                text  \\\n",
       "Index                                                  \n",
       "1       \"George Washington\" was born on  2/22/1732.0   \n",
       "2             \"John Adams\" was born on  10/30/1735.0   \n",
       "\n",
       "                                               embedding  \n",
       "Index                                                     \n",
       "1      [-0.0066544716246426105, 0.0005056292284280062...  \n",
       "2      [-0.0015555905411019921, 0.040288060903549194,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def embed(text):\n",
    "    return client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        ).data[0].embedding\n",
    "\n",
    "df[\"embedding\"] = df['text'].apply(embed)\n",
    "df[[\"text\", \"embedding\"]].to_csv(\"output/presidents_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build your completion function\n",
    "\n",
    "To register our completion function, we need to write a YAML file. You can check [presidents.yaml](./evals_registry/completion_fns/presidents.yaml). \n",
    "\n",
    "Let's explain the file content. RetrievalCompletionFn class takes 3 arguments:\n",
    "1. **completion_fn**: we can simply pass a model name, e.g. 'gpt-4-turbo-preview' or pass another completions function. It is kind of chain, a completion function having another one as an input. \n",
    "2. **embeddings_and_text_path**: CSV file with text and embedding columns. It will be used for retrieval. \n",
    "3. **k**: Top k closest embeddings will be passed to the prompt. In our case it is 2, because the user will always ask questions similar to \"Was Andrew Jackson born before William Harrison?\", so we always need only 2 president.\n",
    "\n",
    "Here we defined 3 different completion functions:\n",
    "1. **cot/gpt-4-turbo-preview**: We will not use this one directly, but use it as an input to the 3rd one. It is based on [ChainOfThoughtCompletionFn](https://github.com/openai/evals/blob/main/evals/completion_fns/cot.py) class, adds chain of thought logic on top of gpt-4-turbo-preview.\n",
    "1. **retrieval/presidents/gpt-4-turbo-preview**: for using 'gpt-4-turbo-preview' with retrieval.\n",
    "2. **retrieval/presidents/cot/gpt-4-turbo-preview**: for using 'gpt-4-turbo-preview' with chain of thought logic, with retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cot/gpt-4-turbo-preview:\n",
      "  class: evals.completion_fns.cot:ChainOfThoughtCompletionFn\n",
      "  args:\n",
      "    cot_completion_fn: gpt-4-turbo-preview\n",
      "\n",
      "retrieval/presidents/gpt-4-turbo-preview:\n",
      "  class: evals.completion_fns.retrieval:RetrievalCompletionFn\n",
      "  args:\n",
      "    completion_fn: gpt-4-turbo-preview\n",
      "    embeddings_and_text_path: ../../output/presidents_embeddings.csv\n",
      "    k: 2\n",
      "\n",
      "retrieval/presidents/cot/gpt-4-turbo-preview:\n",
      "  class: evals.completion_fns.retrieval:RetrievalCompletionFn\n",
      "  args:\n",
      "    completion_fn: cot/gpt-4-turbo-preview\n",
      "    embeddings_and_text_path: ../../output/presidents_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open('evals_registry/completion_fns/presidents.yaml', 'r') as file:\n",
    "    # Read the file's content\n",
    "    file_content = file.read()\n",
    "    # Print the content\n",
    "    print(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build your eval\n",
    "\n",
    "An eval is simply a dataset and a choice of eval class. You have 3 options to build an eval:\n",
    "1. Using one of the basic eval classes. Most common ones are 'Match', 'Includes' and 'FuzyMatch'. For a full list, you can check [here](https://github.com/openai/evals/tree/main/evals/elsuite/basic).\n",
    "2. Using the [model graded eval class](https://github.com/openai/evals/blob/main/docs/eval-templates.md#the-model-graded-eval-template). Model graded eval means using an LLM model to evaluate the outputs of another LLM model. \n",
    "3. Creating your custom eval class. Most of the cases, this will not be necessary. So it is not covered under this notebook. But you can check [here](https://github.com/openai/evals/blob/main/docs/custom-eval.md), if you want to learn more.\n",
    "\n",
    "Register the eval by adding a file to /evals/<eval_name>.yaml under registry folder (in our case it is 'evals_registry') using the elsuite registry format. For example, for a Match eval, it would be:\n",
    "\n",
    "```console\n",
    "<eval_name>:\n",
    "  id: <eval_name>.dev.v0\n",
    "  description: <description>\n",
    "  metrics: [accuracy]\n",
    "\n",
    "<eval_name>.dev.v0:\n",
    "  class: evals.elsuite.basic.match:Match\n",
    "  args:\n",
    "    samples_jsonl: <eval_name>/samples.jsonl\n",
    "```\n",
    "\n",
    "Upon running the eval, the data will be searched for in data folder under registry. For example, if older/samples.jsonl is the provided filepath, the data is expected to be in evals_registry/data/older/samples.jsonl.\n",
    "\n",
    "The naming convention for evals is in the form <eval_name>.<split>.<version>.\n",
    "\n",
    "* <eval_name> is the eval name, used to group evals whose scores are comparable.\n",
    "* <split> is the data split, used to further group evals that are under the same <base_eval>. E.g., \"val\", \"test\", or \"dev\" for testing.\n",
    "* <version> is the version of the eval, which can be any descriptive text you'd like to use (though it's best if it does not contain .).\n",
    "In general, running the same eval name against the same model should always give similar results so that others can reproduce it. Therefore, when you change your eval, you should bump the version.\n",
    "\n",
    "In our sample scerio, we will go ahead with the first option and use 'Match'. You can check [older.yaml](./evals_registry/evals/older.yaml). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "older:\n",
      "  id: older.dev.v0\n",
      "  description: Test the model's ability to determine who is older.\n",
      "  metrics: [accuracy]\n",
      "older.dev.v0:\n",
      "  class: evals.elsuite.basic.match:Match\n",
      "  args:\n",
      "    samples_jsonl: older/older.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode ('r')\n",
    "with open('evals_registry/evals/older.yaml', 'r') as file:\n",
    "    # Read the file's content\n",
    "    file_content = file.read()\n",
    "    # Print the content\n",
    "    print(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace with path to your registry\n",
    "registry_path = \"evals/registry/evals\"\n",
    "os.makedirs(registry_path, exist_ok=True)\n",
    "with open(registry_path + \"/older.yaml\", \"w\") as f:\n",
    "    f.write(registry_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-turbo base: accuracy 0.7\n",
    "!oaieval gpt-4-turbo-preview born-first --max_samples 10 --registry_path ./evals/registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-turbo with retrieval: accuracy 0.9 -> The failure mode here is the retrieved president is incorrect: Andrew Johnson vs Andrew Jackson\n",
    "!oaieval retrieval/presidents/gpt-4-turbo-preview born-first --max_samples 10 --registry_path ./evals/registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3.5-turbo with retrieval and chain-of-thought: accuracy 1.0\n",
    "!oaieval retrieval/presidents/cot/gpt-4-turbo-preview born-first --max_samples 10 --registry_path ./evals/registry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
